# breweries case(Data Pipeline)

## Introdu√ß√£o
O objetivo desse projeto √© criar um pipeline consumindo dados de um API e armazenando em um ambiente Data Lake seguindo a arquitetura Medallion com tr√™s camadas:
- **Bronze üü§:** Dados Brutos.
- **Silver ‚ö™:** Dados selecionados e particionado por localiza√ß√£o.
- **Gold üü°:** Dados agredados para an√°lise.
- **Extra üìä:** Visualiza√ß√£o gr√°fica dos dados que est√£o na camada Gold.

### Feramentas utilizadas 
- **Python** - Linguagem escolhida para a extra√ß√£o e tratamento dos dados entre as camadas do nosso Data Lake.
- **Mage** - Orquestrador do nosso ambiente, nele podemos ter uma vis√£o macro do nosso pipeline e trabalhar com blocos (data loader, transform, data exporter).
- **MinIO** - Reposit√≥rio de dados (Tem o mesmo conceito que o AWS S3). Nele vamos criar nossas camadas do Data Lake que armazenar√£o os dados.
- **Docker** - Conteineriza√ß√£o do nosso ambiente, configurado em um *docker-compose.yml*.

## Arquitetura
![GET](image/arquitetura.png)

## Pr√© Work
Caso queira ver o Pipeline funcionando, antes de clonar o reposit√≥rio, tem alguns requisitos necess√°rio: 
- Instalar Docker (Estou utilizando no linux).
- Instalar Python (Python 3.10.12 or more).
- Criar um arquivo *.env* para incluir as chaves de acesso do MinIO:
  ```bash
      # Definir o nome do usu√°rio e senha que precisa conectar no servi√ßo Web
      MINIO_ACCESS_KEY='<USER>'
      MINIO_SECRET_KEY='<SENHA>'
  ```
- Subir o ambiente:
  ```bash
      # Construir as imagens 
      docker-compose build

      # Subir o ambiente
      docker-compose up -d
  ```

## Extra√ß√£o
Para a extra√ß√£o, foi criado um script python [extract_breweries_data.py](data/data_loaders/extract_breweries_data.py) que consulta os dados da API 
